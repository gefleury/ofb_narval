{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to scrap RPQS from SISPEA and/or Google\n",
    "\n",
    "Notebook to scrap RPQS corresponding to collectivities listed in eg `data/scraping/collectivity_list_for_scraping_example.csv` :\n",
    "- **Scraping from SISPEA works well** but it is still needed to filter out scanned versions, deliberation reports, ... \n",
    "- **Scraping from Google does not work** as it does not filter out PDFs which are not RPQS. However this settles the first step. See comments below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"../\")    # Add the path to the root directory (where we can find the folder narval/)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "from narval.pdfscraper import PDFScraperFromSispea, PDFScraperFromGoogle\n",
    "from narval.utils import get_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = get_data_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping from SISPEA website\n",
    "\n",
    "We scrap RPQS directly from SISPEA website (https://services.eaufrance.fr/) using the template link https://www.services.eaufrance.fr/sispea//referential/download-rpqs.action?collectivityId=206841&rpqsId=512773 (only the `collectivity_id` and `rpqs_id` must be known). Be careful that the fetched PDFs are not necessarily RPQS (there might be also RAD or deliberation reports).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap RPQS from SISPEA website for a given year, competence, status : PDFs are downloaded in `data/scraping/from_sispea/` and the scraping report in `data/scraping` is updated (created if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the input file in data/scraping containing the list of relevant collectivities \n",
    "# (with one municipality, one service per competence)\n",
    "# This file is the output of the notebook \"sispea_extract_data_for_scraping.ipynb\"\n",
    "collectivity_filename = \"collectivity_list_for_scraping_example.csv\"\n",
    "# Name of the output file (scraping report) in data/scraping\n",
    "report_filename = \"scraping_report_from_\"+collectivity_filename\n",
    "\n",
    "# Scraping parameters\n",
    "year = \"2015\"\n",
    "competence = \"assainissement collectif\"\n",
    "status_lot = \"WaitingForInput\"\n",
    "\n",
    "# Scrap PDFs from SISPEA\n",
    "scrapper = PDFScraperFromSispea(collectivity_filename, report_filename)\n",
    "scrapper.download_all_rpqs_from_ids(year=year, competence=competence, status_lot=status_lot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap RPQS from SISPEA website for a given competence and status but all year : PDFs that already exist in `data/scraping/from_sispea/` are not downloaded again. The scraping report is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectivity_filename = \"collectivity_list_for_scraping_20012025.csv\"\n",
    "report_filename = \"scraping_report_from_\"+collectivity_filename\n",
    "\n",
    "# Scraping parameters\n",
    "competence = \"assainissement collectif\"\n",
    "status_lot = \"WaitingForInput\"\n",
    "\n",
    "# Scrap PDFs from SISPEA\n",
    "scrapper = PDFScraperFromSispea(collectivity_filename, report_filename)\n",
    "scrapper.download_all_rpqs_from_ids(competence=competence, status_lot=status_lot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap more PDFs from SISPEA with now the status `Published` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectivity_filename = \"collectivity_list_for_scraping_example.csv\"\n",
    "report_filename = \"scraping_report_from_\"+collectivity_filename\n",
    "\n",
    "# Scraping parameters\n",
    "year = \"2022\"\n",
    "competence = \"assainissement collectif\"\n",
    "status_lot = \"Published\"\n",
    "\n",
    "# Scrap PDFs from SISPEA\n",
    "scrapper = PDFScraperFromSispea(collectivity_filename, report_filename)\n",
    "scrapper.download_all_rpqs_from_ids(year=year, competence=competence, status_lot=status_lot)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**  \n",
    "- There are some `doc`, `rtf`, ... scrapped files. For now, they are not converted to PDFs but downloaded as they are.  \n",
    "- The PDF scrapped files are not necessarily RPQS. There might be also deliberation reports, RAD (OK), or any other reports. Also even if the PDF is a RPQS, it can be a scanned version $\\rightarrow$ a module `pdfclassifier.py` should be added to classify automatically a PDF (Is it a RPQS? Is it a scanned version?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping from Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scrap RPQS from Google :\n",
    "- We search relevant URLS on Google for each collectivity, competence, year by keeping  the first 5 URLs returned from keywords f\"RPQS {collectivity_name} {competence} {year}\", then f\"RAD {collectivity_name} {competence} {year}\" then f\"Rapport du maire {collectivity_name} {competence} {year}\"  \n",
    "- In each url, we look for PDF links  \n",
    "- We download all PDFs (with no filtering) in `data/scraping/from_google` and update the scraping report   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the input file in data/scraping containing the list of relevant collectivities \n",
    "# (with one municipality, one service per competence)\n",
    "# This file is the output of the notebook \"sispea_extract_data_for_scraping.ipynb\"\n",
    "collectivity_filename = \"collectivity_list_for_scraping_example.csv\"\n",
    "report_filename = \"scraping_report_from_\"+collectivity_filename\n",
    "\n",
    "# Scraping parameters\n",
    "year = \"2015\"\n",
    "competence = \"assainissement collectif\"\n",
    "status_lot = \"WaitingForInput\"\n",
    "\n",
    "scrapper = PDFScraperFromGoogle(collectivity_filename, report_filename)\n",
    "scrapper.download_all_pdfs(year=year, competence=competence, status_lot=status_lot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**  \n",
    "- Work in progress, there might be errors. \n",
    "- We have very often a `Request error` (see above), hence many urls are ignored. Why? Can we correct it quickly?\n",
    "- We download far too many PDFs! Most of them are not RPQS.         \n",
    "    - Try to see if keywords must be changed or if we should keep only the 3 (?, instead of 5) top Google results for each query.     \n",
    "    - More importantly, add a module `pdfclassifier.py` to classify PDFs and filter out the ones which are not RPQS (Is it a RPQS? Is it the correct city? Is it the correct competence? Is it the correct year? Is it a scanned version?)\n",
    "- Note that PDFs coming from https://services.eaufrance.fr/ should be immediately filtered out since they are more easily scrapped with `PDFScraperFromSispea`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of scrapped files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(data_dir + \"/data/scraping/from_sispea\")\n",
    "\n",
    "number_files = sum(1 for x in path.rglob('*') if x.is_file() and x.suffix==\".pdf\")\n",
    "print(f\"There are {number_files} PDF files in the folder {path.parent}/{path.name}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(data_dir + \"/data/scraping/from_google\")\n",
    "\n",
    "number_files = sum(1 for x in path.rglob('*') if x.is_file() and x.suffix==\".pdf\")\n",
    "print(f\"There are {number_files} PDF files in the folder {path.parent}/{path.name}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
